<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Gabi Data Science journey  | KNN - Python</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.26" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    <link href='https://gabidan.github.io/dist/main.css' rel='stylesheet' type="text/css" />
    
      
    

    

    <meta property="og:title" content="KNN - Python" />
<meta property="og:description" content="Introduction This report tackles a data analysis problem deriving from the Wine Classification data set published by UC Irvine Machine Learning Repository - https://archive.ics.uci.edu/ml/index.php
Exploratory data analysis I df=pd.read_csv(&#39;wineset.csv&#39;, header=None) df.columns=([&#39;Type&#39;, &#39;Alcohol&#39;, &#39;Malic&#39;, &#39;Ash&#39;, &#39;Alcalinity&#39;, &#39;Magnesium&#39;, &#39;Phenols&#39;, &#39;Flavanoids&#39;, &#39;Nonflavanoids&#39;, &#39;Proanthocyanins&#39;, &#39;Color&#39;, &#39;Hue&#39;, &#39;Dilution&#39;, &#39;Proline&#39;])  df.head()    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    Type Alcohol Malic Ash Alcalinity Magnesium Phenols Flavanoids Nonflavanoids Proanthocyanins Color Hue Dilution Proline     0 1 14." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://gabidan.github.io/posts/my-first-post/" />



<meta property="article:published_time" content="2018-01-29T22:46:52&#43;00:00"/>
<meta property="article:modified_time" content="2018-01-29T22:46:52&#43;00:00"/>











<meta itemprop="name" content="KNN - Python">
<meta itemprop="description" content="Introduction This report tackles a data analysis problem deriving from the Wine Classification data set published by UC Irvine Machine Learning Repository - https://archive.ics.uci.edu/ml/index.php
Exploratory data analysis I df=pd.read_csv(&#39;wineset.csv&#39;, header=None) df.columns=([&#39;Type&#39;, &#39;Alcohol&#39;, &#39;Malic&#39;, &#39;Ash&#39;, &#39;Alcalinity&#39;, &#39;Magnesium&#39;, &#39;Phenols&#39;, &#39;Flavanoids&#39;, &#39;Nonflavanoids&#39;, &#39;Proanthocyanins&#39;, &#39;Color&#39;, &#39;Hue&#39;, &#39;Dilution&#39;, &#39;Proline&#39;])  df.head()    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    Type Alcohol Malic Ash Alcalinity Magnesium Phenols Flavanoids Nonflavanoids Proanthocyanins Color Hue Dilution Proline     0 1 14.">


<meta itemprop="dateModified" content="2018-01-29T22:46:52&#43;00:00" />
<meta itemprop="wordCount" content="1556">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="KNN - Python"/>
<meta name="twitter:description" content="Introduction This report tackles a data analysis problem deriving from the Wine Classification data set published by UC Irvine Machine Learning Repository - https://archive.ics.uci.edu/ml/index.php
Exploratory data analysis I df=pd.read_csv(&#39;wineset.csv&#39;, header=None) df.columns=([&#39;Type&#39;, &#39;Alcohol&#39;, &#39;Malic&#39;, &#39;Ash&#39;, &#39;Alcalinity&#39;, &#39;Magnesium&#39;, &#39;Phenols&#39;, &#39;Flavanoids&#39;, &#39;Nonflavanoids&#39;, &#39;Proanthocyanins&#39;, &#39;Color&#39;, &#39;Hue&#39;, &#39;Dilution&#39;, &#39;Proline&#39;])  df.head()    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    Type Alcohol Malic Ash Alcalinity Magnesium Phenols Flavanoids Nonflavanoids Proanthocyanins Color Hue Dilution Proline     0 1 14."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://gabidan.github.io/" class="f3 fw2 hover-white no-underline white-90 dib">
      Gabi Data Science journey
    </a>
    <div class="flex-l items-center">
      
      








    </div>
  </div>
</nav>

    </div>
  </header>


    <main class="pb7" role="main">
      
  <div class="flex-l mt2 mw8 center">
    <article class="center cf pv5 ph3 ph4-ns mw7">
      <header>
        <p class="f6 b helvetica tracked">
          POSTS
        </p>
        <h1 class="f1">
          KNN - Python
        </h1>
      </header>
      <div class="nested-copy-line-height lh-copy f4 nested-links nested-img mid-gray">
        

<h4 id="introduction">Introduction</h4>

<p>This report tackles a data analysis problem deriving from the Wine Classification data set published by UC Irvine Machine Learning Repository - <a href="https://archive.ics.uci.edu/ml/index.php">https://archive.ics.uci.edu/ml/index.php</a></p>

<h3 id="exploratory-data-analysis-i">Exploratory data analysis I</h3>

<pre><code>df=pd.read_csv('wineset.csv', header=None)
df.columns=(['Type', 'Alcohol', 'Malic', 'Ash', 
                    'Alcalinity', 'Magnesium', 'Phenols', 
                    'Flavanoids', 'Nonflavanoids',
                    'Proanthocyanins', 'Color', 'Hue', 
                    'Dilution', 'Proline'])
</code></pre>

<pre><code>df.head()
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Type</th>
      <th>Alcohol</th>
      <th>Malic</th>
      <th>Ash</th>
      <th>Alcalinity</th>
      <th>Magnesium</th>
      <th>Phenols</th>
      <th>Flavanoids</th>
      <th>Nonflavanoids</th>
      <th>Proanthocyanins</th>
      <th>Color</th>
      <th>Hue</th>
      <th>Dilution</th>
      <th>Proline</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>14.23</td>
      <td>1.71</td>
      <td>2.43</td>
      <td>15.6</td>
      <td>127</td>
      <td>2.80</td>
      <td>3.06</td>
      <td>0.28</td>
      <td>2.29</td>
      <td>5.64</td>
      <td>1.04</td>
      <td>3.92</td>
      <td>1065</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>13.20</td>
      <td>1.78</td>
      <td>2.14</td>
      <td>11.2</td>
      <td>100</td>
      <td>2.65</td>
      <td>2.76</td>
      <td>0.26</td>
      <td>1.28</td>
      <td>4.38</td>
      <td>1.05</td>
      <td>3.40</td>
      <td>1050</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>13.16</td>
      <td>2.36</td>
      <td>2.67</td>
      <td>18.6</td>
      <td>101</td>
      <td>2.80</td>
      <td>3.24</td>
      <td>0.30</td>
      <td>2.81</td>
      <td>5.68</td>
      <td>1.03</td>
      <td>3.17</td>
      <td>1185</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>14.37</td>
      <td>1.95</td>
      <td>2.50</td>
      <td>16.8</td>
      <td>113</td>
      <td>3.85</td>
      <td>3.49</td>
      <td>0.24</td>
      <td>2.18</td>
      <td>7.80</td>
      <td>0.86</td>
      <td>3.45</td>
      <td>1480</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>13.24</td>
      <td>2.59</td>
      <td>2.87</td>
      <td>21.0</td>
      <td>118</td>
      <td>2.80</td>
      <td>2.69</td>
      <td>0.39</td>
      <td>1.82</td>
      <td>4.32</td>
      <td>1.04</td>
      <td>2.93</td>
      <td>735</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">df.describe()
</code></pre>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Type</th>
      <th>Alcohol</th>
      <th>Malic</th>
      <th>Ash</th>
      <th>Alcalinity</th>
      <th>Magnesium</th>
      <th>Phenols</th>
      <th>Flavanoids</th>
      <th>Nonflavanoids</th>
      <th>Proanthocyanins</th>
      <th>Color</th>
      <th>Hue</th>
      <th>Dilution</th>
      <th>Proline</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>178.000000</td>
      <td>178.000000</td>
      <td>178.000000</td>
      <td>178.000000</td>
      <td>178.000000</td>
      <td>178.000000</td>
      <td>178.000000</td>
      <td>178.000000</td>
      <td>178.000000</td>
      <td>178.000000</td>
      <td>178.000000</td>
      <td>178.000000</td>
      <td>178.000000</td>
      <td>178.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>1.938202</td>
      <td>13.000618</td>
      <td>2.336348</td>
      <td>2.366517</td>
      <td>19.494944</td>
      <td>99.741573</td>
      <td>2.295112</td>
      <td>2.029270</td>
      <td>0.361854</td>
      <td>1.590899</td>
      <td>5.058090</td>
      <td>0.957449</td>
      <td>2.611685</td>
      <td>746.893258</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.775035</td>
      <td>0.811827</td>
      <td>1.117146</td>
      <td>0.274344</td>
      <td>3.339564</td>
      <td>14.282484</td>
      <td>0.625851</td>
      <td>0.998859</td>
      <td>0.124453</td>
      <td>0.572359</td>
      <td>2.318286</td>
      <td>0.228572</td>
      <td>0.709990</td>
      <td>314.907474</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.000000</td>
      <td>11.030000</td>
      <td>0.740000</td>
      <td>1.360000</td>
      <td>10.600000</td>
      <td>70.000000</td>
      <td>0.980000</td>
      <td>0.340000</td>
      <td>0.130000</td>
      <td>0.410000</td>
      <td>1.280000</td>
      <td>0.480000</td>
      <td>1.270000</td>
      <td>278.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>1.000000</td>
      <td>12.362500</td>
      <td>1.602500</td>
      <td>2.210000</td>
      <td>17.200000</td>
      <td>88.000000</td>
      <td>1.742500</td>
      <td>1.205000</td>
      <td>0.270000</td>
      <td>1.250000</td>
      <td>3.220000</td>
      <td>0.782500</td>
      <td>1.937500</td>
      <td>500.500000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>2.000000</td>
      <td>13.050000</td>
      <td>1.865000</td>
      <td>2.360000</td>
      <td>19.500000</td>
      <td>98.000000</td>
      <td>2.355000</td>
      <td>2.135000</td>
      <td>0.340000</td>
      <td>1.555000</td>
      <td>4.690000</td>
      <td>0.965000</td>
      <td>2.780000</td>
      <td>673.500000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>3.000000</td>
      <td>13.677500</td>
      <td>3.082500</td>
      <td>2.557500</td>
      <td>21.500000</td>
      <td>107.000000</td>
      <td>2.800000</td>
      <td>2.875000</td>
      <td>0.437500</td>
      <td>1.950000</td>
      <td>6.200000</td>
      <td>1.120000</td>
      <td>3.170000</td>
      <td>985.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>3.000000</td>
      <td>14.830000</td>
      <td>5.800000</td>
      <td>3.230000</td>
      <td>30.000000</td>
      <td>162.000000</td>
      <td>3.880000</td>
      <td>5.080000</td>
      <td>0.660000</td>
      <td>3.580000</td>
      <td>13.000000</td>
      <td>1.710000</td>
      <td>4.000000</td>
      <td>1680.000000</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">df.info()
</code></pre>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 178 entries, 0 to 177
Data columns (total 14 columns):
Type               178 non-null int64
Alcohol            178 non-null float64
Malic              178 non-null float64
Ash                178 non-null float64
Alcalinity         178 non-null float64
Magnesium          178 non-null int64
Phenols            178 non-null float64
Flavanoids         178 non-null float64
Nonflavanoids      178 non-null float64
Proanthocyanins    178 non-null float64
Color              178 non-null float64
Hue                178 non-null float64
Dilution           178 non-null float64
Proline            178 non-null int64
dtypes: float64(11), int64(3)
memory usage: 19.5 KB
</code></pre>

<pre><code class="language-python">df.isnull().sum()
</code></pre>

<pre><code>Type               0
Alcohol            0
Malic              0
Ash                0
Alcalinity         0
Magnesium          0
Phenols            0
Flavanoids         0
Nonflavanoids      0
Proanthocyanins    0
Color              0
Hue                0
Dilution           0
Proline            0
dtype: int64
</code></pre>

<pre><code class="language-python">df.hist()
plt.show()
</code></pre>

<p><img src="/images/output_8_0.png" alt="alt text" /></p>

<pre><code class="language-python">df.plot(kind='density', subplots=True, layout=(4,4), sharex=False)
plt.show()
</code></pre>

<p><img src="/images/output_9_0.png" alt="alt text" /></p>

<pre><code class="language-python">df.plot(kind='box', subplots=True, layout=(3,6), sharex=False, sharey=False)
plt.show()
</code></pre>

<p><img src="/images/output_10_0.png" alt="alt text" /></p>

<ul>
<li>Data frame contains 13 columns and 178 lines</li>
<li>All data entries are numerical - the category (type) of the cultivar is encoded as: 1, 2 or 3</li>
<li>Data frame does not have any empty values</li>
<li>The first column &lsquo;Type&rsquo; contains the classification of the cultivars</li>
<li>Min/Max and Mean values observed above suggest that the scales of measure are varied amongst the attributes, therefore, normalizing/standardizing data potentially would be applied in the following steps in order to produce more meaningful results</li>
<li>&lsquo;Type&rsquo; data is discrete, and the rest of the columns hold continuous data</li>
<li>It is noticeable that the each of the 3 classes of cultivars (&lsquo;Type&rsquo;) hold a very similar number of observations</li>
</ul>

<h4 id="exploratory-data-analysis-ii">Exploratory data analysis II</h4>

<pre><code class="language-python">import seaborn as sns
corr = df[df.columns].corr()
sns.heatmap(corr, cmap=&quot;YlGnBu&quot;, annot = True)
</code></pre>

<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2a3309527f0&gt;
</code></pre>

<p><img src="/images/output_11_1.png" alt="alt text" /></p>

<ul>
<li>It is observed above that cultivar type is least correlated with an attribute &lsquo;Ash&rsquo;</li>
<li>The highest positive correlation is apparent between cultivar type and: &lsquo;Malic&rsquo;, &lsquo;Alcalinity&rsquo;, &lsquo;Nonflavanoids&rsquo; and &lsquo;Color&rsquo;</li>
<li>The highest negative correlation is apparent between cultivar type and: &lsquo;Phenols&rsquo;, &lsquo;Flavanoids&rsquo;, &lsquo;Dilution&rsquo;</li>
<li>It is noticeable that the following attribute pairs are highly positive correlated:</li>
<li>Flavanoids and Phenols</li>
<li>Flavanoids and Dilution</li>
<li>Color and Alhocol</li>
</ul>

<p>We have explored the data distribution of the variables in the analysis above where we have identified the attributes which have outliers as well as fairly normal distribution.</p>

<h4 id="the-methodology-and-methods-applied">The methodology and methods applied</h4>

<p>During the exploratory data analysis we have identified that our data set presents a Supervised Learning problem in Machine Learning terms. We are presented with a property (type of the wine cultivar) and our ultimate aim is to be able to predict the type of the cultivar in instances where the cultivar type label is missing. The core idea behind the methodology of our task is to use an algorithm where the set of independent variables (chemical attributes) will be an input into a choice of function which will map the inputs to the desired outputs corresponding to the type of the cultivar. This iterative process will be carried out until the model achieves a desired level of accuracy on the training data which contains information about the true output.</p>

<p>To narrow it down, our task is to solve a classification problem. A multi-class classification problem, to be precise.</p>

<h4 id="knn">KNN</h4>

<p><em>K- Nearest Neighbours</em></p>

<p>Although used for both: classification and regression, KNN is a very popular and simple algorithm mostly adapted in classification problems.</p>

<p>Firstly, the model was implemented using all but one default parameters, such as the following:</p>

<ul>
<li><p>The distance function is defaulted to minkowski, equivalent to the standard Euclidean distance metric (scikit-learn.org) is the most appropriate distance function to use as all our data is numerical.</p></li>

<li><p>Weights are maintained to default allowing all chosen neighbours an equal contribution. Depending on the performance of the model and the investigation into train/test data split, this parameter can be tuned. As our data set is small and generally well distributed (findings from the exploratory data analysis - we have a very similar number of observations in each class) initially weight adjustments are not made.</p></li>

<li><p>neighbours by default are set to 5. We are working on a small data set, therefore, n is initiated to 3 to avoid overfitting. As this parameter is key in KNN implementation, a cross-validation technique will be used to determine the best number of n.</p></li>
</ul>

<p>Depending on the performance and the nature of this algorithm, the following (in addition to cross-validation) techniques are planned to improve the performance of the model:</p>

<ul>
<li>scaling the data</li>
</ul>

<hr />

<pre><code class="language-python">import pandas as pd
import numpy as np
%matplotlib inline
import matplotlib.pyplot as plt
from pandas.tools.plotting import scatter_matrix
from sklearn.cross_validation import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn import tree
</code></pre>

<p>KNN implementaion
source : <a href="https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/">https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/</a></p>

<p><strong>Step 1 - preparing the data</strong></p>

<ul>
<li><p>The data set will be split into 2 subsets to start with: Train and Validation</p></li>

<li><p>In order to produce more meaningful results this step is performed to set aside a totally unseen data for the cross-validation of the model.</p></li>

<li><p>Train subset will hold 120 observations, which is approx 68% of the orignal data set and will be used to train and test the model.</p></li>

<li><p>Validation subset will be used to perform cross validation.</p></li>
</ul>

<pre><code class="language-python">Train = df.sample(n=120) 

Validation = df.drop(Train.index) 
</code></pre>

<p><strong>Step 2 - building the model</strong></p>

<ul>
<li><p>First of all, data matrix will be assigned to variable X and target vector (response) will be assigned to variable y</p></li>

<li><p>In order to build the model, Train subset will be further split into raining and testing parts, 70% and 30% respectively.</p></li>
</ul>

<pre><code class="language-python"># data matrix X and target vector y
X = np.array(Train.ix[:, 1:13]) 
y = np.array(Train['Type'])

# splitting the data into training and test subsets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
</code></pre>

<pre><code>C:\Users\Gabi\Anaconda3\lib\site-packages\ipykernel_launcher.py:2: DeprecationWarning: 
.ix is deprecated. Please use
.loc for label based indexing or
.iloc for positional indexing

See the documentation here:
http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix
</code></pre>

<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier

# instantiate learning model (k = 3)
knn = KNeighborsClassifier(n_neighbors=3)

# fitting the model
knn.fit(X_train, y_train)

# predict the response
pred = knn.predict(X_test)

# evaluate accuracy
print (accuracy_score(y_test, pred))
</code></pre>

<pre><code>0.861111111111
</code></pre>

<p><strong>Step 3 - experiment with the model on scaled data</strong></p>

<ul>
<li>Data will be scaled in order to see whether this step improves the prediction accuracy</li>
</ul>

<pre><code class="language-python">#Fitting the model on scaled data 
from sklearn.preprocessing import scale
Xs = scale(X)
Xs_train, Xs_test, y_train, y_test = train_test_split(Xs, y, test_size=0.2, random_state=42)
</code></pre>

<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier

# instantiate learning model (k = 3)
knn1 = KNeighborsClassifier(n_neighbors=3)

# fitting the model
knn1.fit(Xs_train, y_train)

# predict the response
pred = knn1.predict(Xs_test)

# evaluate accuracy
print (accuracy_score(y_test, pred))
</code></pre>

<pre><code>0.916666666667
</code></pre>

<p><strong>Step 4 - Cross-validation I</strong></p>

<ul>
<li>We will be splitting the Validation subset, fitting a model and computing the score 10 consecutive times (cv=10)<br /></li>
</ul>

<pre><code class="language-python">#Performing cross validation on the unseen Validation subset, using unscaled data  

Xval = np.array(Validation.ix[:, 1:13]) 
yval = np.array(Validation['Type'])

</code></pre>

<pre><code class="language-python">knn_crs = KNeighborsClassifier(n_neighbors=3)
Score = cross_val_score(knn_crs, Xval, yval, cv=10, scoring=&quot;accuracy&quot;)

print(Score)  #return a np.array of each test trill
print(Score.mean()) #return the average of the Score/out-of-sample accuracy
</code></pre>

<pre><code>[ 0.28571429  0.71428571  0.85714286  0.33333333  1.          0.66666667
  0.6         1.          0.8         0.75      ]
0.700714285714
</code></pre>

<p><strong>Step 5 - Cross-validation II</strong></p>

<ul>
<li><p>We will be looking for an optimum number of neighbors during the cross-validation process</p></li>

<li><p>The number of neighbors will be stored in a variable <em>nb_list_odd</em>: off numbers between 1 and 50</p></li>
</ul>

<pre><code class="language-python">nb_list=list(range(1,50))
nb_list_odd=list(filter(lambda x: x % 2 != 0, myList))
</code></pre>

<pre><code class="language-python">cv_scores=[]
for k in nb_list_odd:
    knn_crs_n = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn_crs_n, Xval, yval, cv=10, scoring='accuracy')
    cv_scores.append(scores.mean())
</code></pre>

<pre><code class="language-python"># changing to misclassification error
MSE = [1 - x for x in cv_scores]

# determining best k
optimal_k = neighbors[MSE.index(min(MSE))]
print (&quot;The optimal number of neighbors is %d&quot; % optimal_k)

# plot misclassification error vs k
plt.plot(neighbors, MSE)
plt.xlabel('Number of Neighbors K')
plt.ylabel('Misclassification Error')
plt.show()
</code></pre>

<pre><code>The optimal number of neighbors is 1
</code></pre>

<p><img src="/images/output_17_1.png" alt="alt text" /></p>

      </div>
    </article>
    <aside class="ph3 mt2 mt6-ns">
      







  <div class="bg-light-gray pa3">
    <ul>
      <li class="list b mb3">
        2 More Posts
      </li>
      
        <li class="list f5 w-100 hover-bg-white nl1">
          
          <a href="/posts/my-3rd-post/" class="link ph2 pv2 db black">
            DecisionTree - Python
          </a>
        </li>
      
        <li class="list f5 w-100 hover-bg-white nl1">
          
          <a href="/posts/my-2nd-post/" class="link ph2 pv2 db black">
            Deep learning and Neural Networks
          </a>
        </li>
      
        <li class="list f5 w-100 hover-bg-white nl1">
          
          <a href="/posts/my-first-post/" class="link ph2 pv2 db black o-50">
            KNN - Python
          </a>
        </li>
      
    </ul>
  </div>


    </aside>
  </div>

    </main>
    <footer class="bg-near-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://gabidan.github.io/" >
    &copy; 2018 Gabi Data Science journey
  </a>
  








  </div>
</footer>

    <script src="https://gabidan.github.io/dist/app.bundle.js" async></script>

  </body>
</html>
