<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Gabi Data Science journey  | DecisionTree - Python</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.26" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    <link href='https://gabidan.github.io/dist/main.css' rel='stylesheet' type="text/css" />
    
      
    

    

    <meta property="og:title" content="DecisionTree - Python" />
<meta property="og:description" content="Following the initial exploration of the Wine Data set this post uses the DecisionTreeClassifier from Python&rsquo;s scikit-learn to predict 3 different wine cultivars.
Source: https://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/
Step 1 - preparing the data
 Creating Pandas data frame  df=pd.read_csv(&#39;wineset.csv&#39;, header=None) df.columns=([&#39;Type&#39;, &#39;Alcohol&#39;, &#39;Malic&#39;, &#39;Ash&#39;, &#39;Alcalinity&#39;, &#39;Magnesium&#39;, &#39;Phenols&#39;, &#39;Flavanoids&#39;, &#39;Nonflavanoids&#39;, &#39;Proanthocyanins&#39;, &#39;Color&#39;, &#39;Hue&#39;, &#39;Dilution&#39;, &#39;Proline&#39;])   Spliting the data into feature set (x) and target set (y)  x = df.values[:, 1:13] y = df." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://gabidan.github.io/posts/my-3rd-post/" />



<meta property="article:published_time" content="2018-02-04T19:50:58&#43;00:00"/>
<meta property="article:modified_time" content="2018-02-04T19:50:58&#43;00:00"/>











<meta itemprop="name" content="DecisionTree - Python">
<meta itemprop="description" content="Following the initial exploration of the Wine Data set this post uses the DecisionTreeClassifier from Python&rsquo;s scikit-learn to predict 3 different wine cultivars.
Source: https://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/
Step 1 - preparing the data
 Creating Pandas data frame  df=pd.read_csv(&#39;wineset.csv&#39;, header=None) df.columns=([&#39;Type&#39;, &#39;Alcohol&#39;, &#39;Malic&#39;, &#39;Ash&#39;, &#39;Alcalinity&#39;, &#39;Magnesium&#39;, &#39;Phenols&#39;, &#39;Flavanoids&#39;, &#39;Nonflavanoids&#39;, &#39;Proanthocyanins&#39;, &#39;Color&#39;, &#39;Hue&#39;, &#39;Dilution&#39;, &#39;Proline&#39;])   Spliting the data into feature set (x) and target set (y)  x = df.values[:, 1:13] y = df.">


<meta itemprop="dateModified" content="2018-02-04T19:50:58&#43;00:00" />
<meta itemprop="wordCount" content="575">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="DecisionTree - Python"/>
<meta name="twitter:description" content="Following the initial exploration of the Wine Data set this post uses the DecisionTreeClassifier from Python&rsquo;s scikit-learn to predict 3 different wine cultivars.
Source: https://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/
Step 1 - preparing the data
 Creating Pandas data frame  df=pd.read_csv(&#39;wineset.csv&#39;, header=None) df.columns=([&#39;Type&#39;, &#39;Alcohol&#39;, &#39;Malic&#39;, &#39;Ash&#39;, &#39;Alcalinity&#39;, &#39;Magnesium&#39;, &#39;Phenols&#39;, &#39;Flavanoids&#39;, &#39;Nonflavanoids&#39;, &#39;Proanthocyanins&#39;, &#39;Color&#39;, &#39;Hue&#39;, &#39;Dilution&#39;, &#39;Proline&#39;])   Spliting the data into feature set (x) and target set (y)  x = df.values[:, 1:13] y = df."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://gabidan.github.io/" class="f3 fw2 hover-white no-underline white-90 dib">
      Gabi Data Science journey
    </a>
    <div class="flex-l items-center">
      
      








    </div>
  </div>
</nav>

    </div>
  </header>


    <main class="pb7" role="main">
      
  <div class="flex-l mt2 mw8 center">
    <article class="center cf pv5 ph3 ph4-ns mw7">
      <header>
        <p class="f6 b helvetica tracked">
          POSTS
        </p>
        <h1 class="f1">
          DecisionTree - Python
        </h1>
      </header>
      <div class="nested-copy-line-height lh-copy f4 nested-links nested-img mid-gray">
        <p>Following the initial exploration of the Wine Data set this post uses the DecisionTreeClassifier from Python&rsquo;s scikit-learn to predict 3 different wine cultivars.</p>

<p>Source: <a href="https://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/">https://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/</a></p>

<p><strong>Step 1 - preparing the data</strong></p>

<ul>
<li>Creating Pandas data frame</li>
</ul>

<pre><code class="language-python">df=pd.read_csv('wineset.csv', header=None)
df.columns=(['Type', 'Alcohol', 'Malic', 'Ash', 
                    'Alcalinity', 'Magnesium', 'Phenols', 
                    'Flavanoids', 'Nonflavanoids',
                    'Proanthocyanins', 'Color', 'Hue', 
                    'Dilution', 'Proline'])
                    

</code></pre>

<ul>
<li>Spliting the data into feature set (x) and target set (y)</li>
</ul>

<pre><code class="language-python">x = df.values[:, 1:13]
y = df.values[:,0]
</code></pre>

<ul>
<li><p>Diving the data into training and test subsets.</p></li>

<li><p>The parameter test_size is given value 0.3; it means test sets will be 30% of whole dataset  &amp; training dataset’s size will be 70% of the entire dataset.</p></li>
</ul>

<pre><code class="language-python">x_train, x_test, y_train, y_test = train_test_split( x, y, test_size = 0.3, random_state = 10)
</code></pre>

<p><strong>Step 2 - fitting the model</strong></p>

<ul>
<li><p>The root node (the first decision node) partitions the data based on the most influential feature partitioning using either gini or entropy measure.</p></li>

<li><p>Generally, the performance does not change when using one or the other. Laura Elena Raileanu and Kilian Stoffel &ldquo;Theoretical comparison between the gini index and information gain criteria&rdquo; noted these remarks:</p>

<ul>
<li><p>It only matters in 2% of the cases whether you use gini impurity or entropy.</p></li>

<li><p>Entropy might be a little slower to compute (because it makes use of the logarithm)</p></li>
</ul></li>
</ul>

<p>*Decision Tree Classifier with criterion <strong>gini index</strong>*</p>

<pre><code class="language-python">clf_gini = DecisionTreeClassifier(criterion = &quot;gini&quot;, random_state = 100,
                               max_depth=3, min_samples_leaf=5)
clfgini=clf_gini.fit(x_train, y_train)
</code></pre>

<p>*Decision Tree Classifier with criterion <strong>information gain (entropy)</strong>*</p>

<pre><code class="language-python">clf_entropy = DecisionTreeClassifier(criterion = &quot;entropy&quot;, random_state = 100,
max_depth=3, min_samples_leaf=5)
clf_entropy.fit(x_train, y_train)
</code></pre>

<p>Now, we have modeled 2 classifiers. One classifier with gini index &amp; another one with information gain as the criterion. We are ready to predict classes for our test set. We can use predict() method. Let’s try to predict target variable for test set’s 1st record.</p>

<pre><code class="language-python">#predict for the whle test data set - Prediction for Decision Tree classifier with criterion as gini index
y_pred = clf_gini.predict(x_test)
y_pred
</code></pre>

<pre><code>array([ 2.,  2.,  1.,  2.,  1.,  2.,  2.,  1.,  3.,  1.,  1.,  1.,  1.,
        3.,  2.,  2.,  3.,  2.,  2.,  3.,  1.,  3.,  1.,  1.,  1.,  3.,
        2.,  3.,  2.,  1.,  3.,  3.,  2.,  3.,  2.,  1.,  2.,  2.,  2.,
        2.,  1.,  1.,  3.,  1.,  1.,  2.,  3.,  1.,  2.,  1.,  3.,  2.,
        3.,  1.])
</code></pre>

<pre><code class="language-python">#predict for the whole test data set - Prediction for Decision Tree classifier with criterion as information gain 
y_pred_en = clf_entropy.predict(x_test)
y_pred_en
</code></pre>

<pre><code>array([ 2.,  2.,  1.,  2.,  1.,  2.,  2.,  1.,  3.,  1.,  1.,  1.,  1.,
        3.,  2.,  2.,  1.,  2.,  2.,  3.,  1.,  3.,  1.,  1.,  2.,  2.,
        2.,  3.,  2.,  1.,  3.,  3.,  2.,  3.,  2.,  1.,  2.,  2.,  2.,
        2.,  2.,  1.,  3.,  2.,  1.,  2.,  3.,  2.,  2.,  1.,  1.,  2.,
        3.,  1.])
</code></pre>

<p>The function accuracy_score() will be used to print accuracy of Decision Tree algorithm. By accuracy, we mean the ratio of the correctly predicted data points to all the predicted data points. Accuracy as a metric helps to understand the effectiveness of our algorithm. It takes 4 parameters.</p>

<ul>
<li>y_true,</li>
<li>y_pred,</li>
<li>normalize,</li>
<li>sample_weight.</li>
</ul>

<p>Out of these 4, normalize &amp; sample_weight are optional parameters. The parameter y_true  accepts an array of correct labels and y_pred takes an array of predicted labels that are returned by the classifier. It returns accuracy as a float value.</p>

<pre><code class="language-python">#Accuracy for Decision Tree classifier with criterion as gini index
print (&quot;Accuracy is &quot;, accuracy_score(y_test,y_pred)*100)
</code></pre>

<pre><code>Accuracy is  83.3333333333
</code></pre>

<pre><code class="language-python">#Accuracy for Decision Tree classifier with criterion as information gain
print (&quot;Accuracy is &quot;, accuracy_score(y_test,y_pred_en)*100)
</code></pre>

<pre><code>Accuracy is  87.037037037
</code></pre>

      </div>
    </article>
    <aside class="ph3 mt2 mt6-ns">
      







  <div class="bg-light-gray pa3">
    <ul>
      <li class="list b mb3">
        5 More Posts
      </li>
      
        <li class="list f5 w-100 hover-bg-white nl1">
          
          <a href="/posts/my-6th-post/" class="link ph2 pv2 db black">
            Cross-Validation: k-folds
          </a>
        </li>
      
        <li class="list f5 w-100 hover-bg-white nl1">
          
          <a href="/posts/my-5th-post/" class="link ph2 pv2 db black">
            Web scraping using BeautifulSoup
          </a>
        </li>
      
        <li class="list f5 w-100 hover-bg-white nl1">
          
          <a href="/posts/my-4th-post/" class="link ph2 pv2 db black">
            Neural Network - Python
          </a>
        </li>
      
        <li class="list f5 w-100 hover-bg-white nl1">
          
          <a href="/posts/my-3rd-post/" class="link ph2 pv2 db black o-50">
            DecisionTree - Python
          </a>
        </li>
      
        <li class="list f5 w-100 hover-bg-white nl1">
          
          <a href="/posts/my-2nd-post/" class="link ph2 pv2 db black">
            Deep learning and Neural Networks
          </a>
        </li>
      
        <li class="list f5 w-100 hover-bg-white nl1">
          
          <a href="/posts/my-first-post/" class="link ph2 pv2 db black">
            KNN - Python
          </a>
        </li>
      
    </ul>
  </div>


    </aside>
  </div>

    </main>
    <footer class="bg-near-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://gabidan.github.io/" >
    &copy; 2018 Gabi Data Science journey
  </a>
  








  </div>
</footer>

    <script src="https://gabidan.github.io/dist/app.bundle.js" async></script>

  </body>
</html>
